\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[francais]{babel}
\usepackage{amsmath, amssymb}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\ra\rightarrow
\newcommand\la\leftarrow
\newcommand\lra\leftrightarrow
\newcommand\ua\uparrow
\newcommand\da\downarrow
\newcommand\uda\updownarrow
\newcommand\nea\nearrow

\newcommand\Ra\Rightarrow
\newcommand\La\Leftarrow
\newcommand\Lra\Leftrightarrow

\newcommand\mat[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand\smat[1]{\setstretch{1}{\ensuremath{\scalefont{0.8}\mat{#1}}}}

\newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
\newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
\newcommand{\bracket}[2]{\ensuremath{\left\langle #1 \middle| #2 \right\rangle}}
\newcommand{\matrixel}[3]{\ensuremath{\left\langle #1 \middle| #2 \middle| #3 \right\rangle}}
\newenvironment{eq*}{\begin{equation*}\begin{gathered}}{\end{gathered}\end{equation*}}
\newenvironment{eqs*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}

% Paragraph formatting
\setlength{\parindent}{0pt}
\usepackage{setspace}
\onehalfspacing
% Margin
\usepackage[margin=1in]{geometry}
\setcounter{secnumdepth}{0}

% Doc info
\author{Olivier St-Laurent, Maxime Daigle}
\title{IFT-3395  Devoir 2}
\date{2018-10-15}

\begin{document}

\maketitle

\section{Question 1.1 Régression linéaire}

\subsection{1. Quel est l'ensemble $\theta$ des paramètres? Donner la nature et la dimension de chacun}

	$\theta = \{ w, b\}$ où w est le vecteurs des poids donnés à chaque 'features' et b est le biais. 
	\begin{eqs*}
		\theta \in \mathbb{R}^{d+1}, w \in \mathbb{R}^{d}, b \in \mathbb{R}
	\end{eqs*}

\subsection{2. Habituellement, le coût utilisé pour une régresion linéaire est l'erreur quadratique. Le rique empirique $\hat{R}$ sur $D_{n}$ est la somme
des coûts sur $D_{n}$. Donner l'expréssion mathématique de ce risque empirique.}
	
	\begin{eqs*}	
		\hat{R} = \sum_{i=1}^{n} L((x^{(i)}, t^{(i)}), f )= \sum_{i=1}^{n} (f(x^{(i)}) - t^{(i)})^{2}
	\end{eqs*}

\subsection{3. Pour minimiser le risque empirique, on cherche la valeur des paramètres qui donne le moins d'erreur sur l'ensemble d'entraînement.
Exprimer ce problème de minimisation}

	\[\theta^{*} = \argmin_{\theta} \hat{R} = \argmin_{\theta} \sum_{i=1}^{n} (f_{\theta}(x^{(i)}) - t^{(i)})^{2}\] \\
	Avec la régression linéaire, il suffit de résoudre analytiquement :	
	\begin{eqs*}	
		\nabla \hat{R}(\theta) = \frac{\partial\hat{R}}{\partial \theta}(\theta^{*}) = 0
	\end{eqs*}


\subsection{4. Exprimer le gradient du risque empirique}
	\begin{align}
	\nabla \hat{R}(\theta) = \frac{\partial\hat{R}}{\partial\theta}(\theta) &= \begin{bmatrix}    														        													\frac{\partial\hat{R}}{\partial\theta_{0}}(\theta) \\
           														\frac{\partial\hat{R}}{\partial\theta_{1}}(\theta) \\
															\vdots \\
           														\frac{\partial\hat{R}}{\partial\theta_{d}}(\theta)
         														\end{bmatrix}
	\end{align}
	
	\begin{eqs*}	
		= \frac{\partial}{\partial\theta} (\sum_{i=1}^{n} (f_{\theta}(x^{(i)}) - t^{(i)})^{2})
	\end{eqs*}

	\begin{eqs*}
		= \frac{\partial}{\partial\theta} (\sum_{i=1}^{n} (\begin{bmatrix}
									\theta_{1} \\
									\theta_{2} \\
									\vdots \\
									\theta_{d}
									\end{bmatrix}^{T}x^{(i)} + \theta_{0} - t^{(i)})^{2})
	\end{eqs*}

	\begin{eqs*}
		= \frac{\partial}{\partial\theta} (\sum_{i=1}^{n}(w^{T}x^{(i)} + b - t^{(i)})^{2})
	\end{eqs*}

	\begin{eqs*}
		=  \begin{bmatrix}
			\sum_{i=1}^{n} 2(w^{T}x^{(i)} + b - t^{(i)}) \\
			\sum_{i=1}^{n} 2(w^{T}x^{(i)} + b - t^{(i)})x_{1}^{(i)} \\
			\sum_{i=1}^{n} 2(w^{T}x^{(i)} + b - t^{(i)})x_{2}^{(i)} \\
			\vdots \\
			\sum_{i=1}^{n} 2(w^{T}x^{(i)} + b - t^{(i)})x_{d}^{(i)}
			\end{bmatrix}
	\end{eqs*}
	

\subsection{5. Expliquer la relation entre le gradient du risque empirique et les erreurs du modèle sur l'ensemble d'entraînement}
	Le gradient du risque empirique permet de trouver le minimum global des erreurs du modèle sur l'ensemble d'entraînement. Autrement dit, vu que ce cas-ci est convexe ,le point où le gradient est égal à 0 est le point où les erreurs du modèle sont les plus petites sur l'ensemble d'entraînement.


\section{Question 1.2 Régression linéaire régularisée (“ridge regression”)}

\subsection{1. Exprimez le gradient du risque régularisé. En quoi diffère-t-il du gradient du risque empirique non régularisé?}

	\begin{eqs*}
		\nabla\widetilde{R}(\theta) = \frac{\partial\widetilde{R}(\theta)}{\partial\theta}
	\end{eqs*}

	\begin{eqs*}
		= \frac{\partial}{\partial\theta}(\hat{R} + \lambda\mathcal{L}(\theta))
	\end{eqs*}

	\begin{eqs*}
		= \frac{\partial}{\partial\theta}(\sum_{i=1}^{n}(w^{T}x^{(i)} + b - t^{(i)})^{2} + \lambda\sum_{k=1}^{d} w_{k}^{2})
	\end{eqs*}

	\begin{eqs*}
		= \frac{\partial}{\partial\theta}(\sum_{i=1}^{n}(w^{T}x^{(i)} + b - t^{(i)})^{2}) + \frac{\partial}{\partial\theta}(\lambda\sum_{k=1}^{d} w_{k}^{2})
	\end{eqs*}

	\begin{eqs*}
		= \nabla\hat{R}(\theta) + \frac{\partial}{\partial\theta}\lambda(\sum_{k=1}^{d} w_{k}^{2})
	\end{eqs*}	
	
	Donc, la différence entre $\nabla\widetilde{R}(\theta)$ et $\nabla\hat{R}(\theta)$ est l'addition de $\frac{\partial}{\partial\theta}\lambda(\sum_{k=1}^{d} w_{k}^{2})$


\subsection{2. Donner le pseudo code détaillé de l'algorithme qui cherche les paramètres optimaux pour minimiser $\widetilde{R}$ par descente de gradient batch.}
	
$\theta$ = initialize\_randomly() \\
stop = False \\
while not stop: \\
\-\hspace{1cm} gradient = $\sum_{i=1}^{n} \frac{\partial}{\partial\theta}(w^{T}x^{(i)} + b - t^{(i)})^{2} + \lambda \frac{\partial}{\partial\theta}(\sum_{k=1}^{d} w_{k}^{2})$ \\
\-\hspace{1cm} $\theta =\theta - \eta.\text{gradient}$ \\
\-\hspace{1cm} stop = || gradient || < $\varepsilon$

\subsection{3. Exprimer le risque empirique et son gradient sous forme matricielle}
	\begin{eqs*} 
		\hat{R} = (Xw-t)^{2} \\
		\nabla\hat{R}(\theta) = 2(X^{T}(Xw-t))
	\end{eqs*}
\subsection{4. Donner la solution analytique sous forme matricielle}
	\begin{eqs*}
		\theta^{*} = (X^{T}X)^{-1}X^{T}t
	\end{eqs*}


\section{Question 1.3 Régression avec un pré-traitement non-linéaire fixe}

\subsection{1. Écriver de manière détaillée la from paramétrique qu'on obtient pour $\widetilde{f}(x)$ dans le cas une dimension $(x \in \mathbb{R})$ si 
on utilise $\phi = \phi_{poly^{k}}$}

	\begin{eqs*}
		\widetilde{f}(x) = f(\phi_{poly^{k}}(x)) = w^{T} \phi_{poly^{k}}(x) + b = w^{T}\begin{bmatrix}
																			x \\
																			x^{2} \\
																			\vdots \\
																			x^{k}
																			\end{bmatrix} + b
	\end{eqs*}


\subsection{2. Préciser quels sont les paramètres et leur dimensionalité}

Les paramètres sont \textbf{w} et \textbf{b}. Pour $\phi_{poly^{k}} \in \mathbb{R}^{k}$, $w \in \mathbb{R}^{k}$ est un vecteur. $b \in \mathbb{R}$ est un scalaire.






















\end{document}