\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[francais]{babel}
\usepackage{amsmath, amssymb}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\ra\rightarrow
\newcommand\la\leftarrow
\newcommand\lra\leftrightarrow
\newcommand\ua\uparrow
\newcommand\da\downarrow
\newcommand\uda\updownarrow
\newcommand\nea\nearrow

\newcommand\Ra\Rightarrow
\newcommand\La\Leftarrow
\newcommand\Lra\Leftrightarrow

\newcommand\mat[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand\smat[1]{\setstretch{1}{\ensuremath{\scalefont{0.8}\mat{#1}}}}

\newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
\newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
\newcommand{\bracket}[2]{\ensuremath{\left\langle #1 \middle| #2 \right\rangle}}
\newcommand{\matrixel}[3]{\ensuremath{\left\langle #1 \middle| #2 \middle| #3 \right\rangle}}
\newenvironment{eq*}{\begin{equation*}\begin{gathered}}{\end{gathered}\end{equation*}}
\newenvironment{eqs*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}

% Paragraph formatting
\setlength{\parindent}{0pt}
\usepackage{setspace}
\onehalfspacing
% Margin
\usepackage[margin=1in]{geometry}
\setcounter{secnumdepth}{0}

% Doc info
\author{Olivier St-Laurent, Maxime Daigle}
\title{IFT-3395  Devoir 3}
\date{2018-11-09}

\begin{document}

\maketitle

\section{Question 1 Relations et dérivées de quelques fonction de base}

\subsection{1. Montrez que sigmoid($x$) = $\frac{1}{2}(tanh(\frac{1}{2}x) + 1)$}

	$ sigmoid(x) = \frac{1}{2}(tanh(\frac{1}{2}x) + 1)  \iff  2sigmoid(x) - 1 = tanh(\frac{x}{2}) $ \\
\\
	$ 2sigmoid(x) - 1 = \frac{2-1-exp(-x)}{1 + exp(-x)} = \frac{1 - exp(-x)}{1+exp(x)}  = \frac{exp(\frac{x}{2})}{exp(\frac{x}{2})} (\frac{1 - exp(-x)}{1+exp(-x)})$ \\
\\
	$ = \frac{exp(\frac{x}{2}) - exp(\frac{x}{2})exp(-x)}{exp(\frac{x}{2}) + exp(\frac{x}{2})exp(-x)} = \frac{exp(\frac{x}{2}) - exp(\frac{-x}{2})} {exp(\frac{x}{2}) + exp(\frac{-x}{2})} = tanh(\frac{x}{2})$

\subsection{2. Montez que $\ln sigmoid(x)$ = $-softplus(-x)$}
	$ \ln sigmoid(x) = \ln \frac{1}{1 + exp(-x)} = \ln(1) - \ln(1+exp(-x)) = 0 + \ln (1+exp(-x)) = -softplus(-x)$
	
\subsection{3. Montrez que $\frac{d\text{ }sigmoid}{dx}(x) = sigmoid(x)(1-sigmoid(x)$}
	$ \frac{d\text{ }sigmoid}{dx}(x) = \frac{((1+exp(-x))^{-1})}{dx} =  \frac{-1}{(1 + e^{-x})^{2}}(-e^{-x})  = (\frac{1}{1 + e^{-x}})(\frac{e^{-x}}{1+e^{-x}})$ \\
\\
$ = sigmoid(x)(\frac{1 + e^{-x}}{1 + e^{-x}} - \frac{1}{1 + e^{-x}}) =  sigmoid(x)(1 - sigmoid(x))$

\subsection{4. Montrez que la dérivée de tanh est : $tanh'(x) = 1 - tanh^{2}(x)$}
	$ \frac{d}{dx}(\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}) =  \frac{(e^{x} - e^{-x})'(e^{x} + e^{-x})-(e^{x} - e^{-x})(e^{x} + e^{-x})'}{(e^{x}e^{-x})^{2}} = \frac{(e^{x}+e^{-x})^{2} - (e^{x}-e^{-x})^{2}}{(e^{x}+e^{-x})^{2}}$ \\
\\
$ = \frac{(e^{x}+e^{-x})^{2}}{(e^{x}+e^{-x})^{2}} - \frac{(e^{x}-e^{-x})^{2}}{(e^{x} + e^{-x})^{2}} = 1 - tanh^{2}(x)$

\subsection{5. Expliquer la relation entre le gradient du risque empirique et les erreurs du modèle sur l'ensemble d'entraînement}
	Le gradient du risque empirique permet de trouver le minimum global des erreurs du modèle sur l'ensemble d'entraînement. Autrement dit,
	le point où le gradient est égal à 0 est le point où les erreurs du modèle sont les plus petites sur l'ensemble d'entraînement.


\section{Question 1.2 Régression linéaire régularisée (“ridge regression”)}

\subsection{1. Exprimez le gradient du risque régularisé. En quoi diffère-t-il du gradient du risque empirique non régularisé?}

	\begin{eqs*}
		\nabla\widetilde{R}(\theta) = \frac{\delta\widetilde{R}(\theta)}{\delta\theta}
	\end{eqs*}

	\begin{eqs*}
		= \frac{\delta}{\delta\theta}(\hat{R} + \lambda\mathcal{L}(\theta))
	\end{eqs*}

	\begin{eqs*}
		= \frac{\delta}{\delta\theta}(\sum_{i=1}^{n}(w^{T}x^{(i)} + b - t^{(i)})^{2} + \lambda\sum_{k=1}^{d} w_{k}^{2})
	\end{eqs*}

	\begin{eqs*}
		= \frac{\delta}{\delta\theta}(\sum_{i=1}^{n}(w^{T}x^{(i)} + b - t^{(i)})^{2}) + \frac{\delta}{\delta\theta}(\lambda\sum_{k=1}^{d} w_{k}^{2})
	\end{eqs*}

	\begin{eqs*}
		= \nabla\hat{R}(\theta) + \frac{\delta}{\delta\theta}\lambda(\sum_{k=1}^{d} w_{k}^{2})
	\end{eqs*}	
	
	Donc, la différence entre $\nabla\widetilde{R}(\theta)$ et $\nabla\hat{R}(\theta)$ est l'addition de $\frac{\delta}{\delta\theta}\lambda(\sum_{k=1}^{d} w_{k}^{2})$


\subsection{2. Donner le pseudo code détaillé de l'algorithme qui cherche les paramètres optimaux pour minimiser $\widetilde{R}$ par descente de gradient batch.}
	\begin{eqs*}
		\text{loop } \theta \leftarrow \theta - \eta \frac{\delta\widetilde{R}}{\delta \theta}
	\end{eqs*}	

	\begin{eqs*}
		 = \theta - \eta[\sum_{i=1}^{n} \frac{\delta}{\delta\theta}(w^{T}x^{(i)} + b - t^{(i)})^{2} + \lambda \frac{\delta}{\delta\theta}(\sum_{k=1}^{d} w_{k}^{2})]
	\end{eqs*}

L'algorithme est donc: \\
$\theta$ = initialize\_randomly() \\
stop = False \\
while not stop: \\
\-\hspace{1cm} gradient = $\sum_{i=1}^{n} \frac{\delta}{\delta\theta}(w^{T}x^{(i)} + b - t^{(i)})^{2} + \lambda \frac{\delta}{\delta\theta}(\sum_{k=1}^{d} w_{k}^{2})$ \\
\-\hspace{1cm} $\theta =\theta - \eta.\text{gradient}$ \\
\-\hspace{1cm} stop = || gradient || < $\varepsilon$

\subsection{3.exprimer le risque empirique et son gradient sous forme matricielle}
	\begin{eqs*}
		\theta^{*} = (X^{T}X)^{-1}X^{T}t
	\end{eqs*}


\section{Question 1.3 Régression avec un pré-traitement non-linéaire fixe}

\subsection{1. Écriver de manière détaillée la from paramétrique qu'on obtient pour $\widetilde{f}(x)$ dans le cas une dimension $(x \in \mathbb{R})$ si 
on utilise $\phi = \phi_{poly^{k}}$}

	\begin{eqs*}
		\widetilde{f}(x) = f(\phi_{poly^{k}}(x)) = w^{T} \phi_{poly^{k}}(x) + b = w^{T}\begin{bmatrix}
																			x \\
																			x^{2} \\
																			\vdots \\
																			x^{k}
																			\end{bmatrix} + b
	\end{eqs*}


\subsection{2. Préciser quels sont les paramètres et leur dimensionalité}

Les paramètres sont \textbf{w} et \textbf{b}. Pour $\phi_{poly^{k}} \in \mathbb{R}^{k}$, $w \in \mathbb{R}^{k}$ est un vecteur. $b \in mathbb{R}$ est un scalaire.






















\end{document}