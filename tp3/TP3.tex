\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[francais]{babel}
\usepackage{amsmath, amssymb}
\usepackage{amsmath}
\usepackage{arydshln}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\ra\rightarrow
\newcommand\la\leftarrow
\newcommand\lra\leftrightarrow
\newcommand\ua\uparrow
\newcommand\da\downarrow
\newcommand\uda\updownarrow
\newcommand\nea\nearrow
\newcommand{\addsum}[1]{%
  \mathstrut\smash{\begin{array}[t]{@{}r@{}}#1\end{array}}%
}

\newcommand\Ra\Rightarrow
\newcommand\La\Leftarrow
\newcommand\Lra\Leftrightarrow

\newcommand\mat[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand\smat[1]{\setstretch{1}{\ensuremath{\scalefont{0.8}\mat{#1}}}}

\newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
\newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
\newcommand{\bracket}[2]{\ensuremath{\left\langle #1 \middle| #2 \right\rangle}}
\newcommand{\matrixel}[3]{\ensuremath{\left\langle #1 \middle| #2 \middle| #3 \right\rangle}}
\newenvironment{eq*}{\begin{equation*}\begin{gathered}}{\end{gathered}\end{equation*}}
\newenvironment{eqs*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}

% Paragraph formatting
\setlength{\parindent}{0pt}
\usepackage{setspace}
\onehalfspacing
% Margin
\usepackage[margin=1in]{geometry}
\setcounter{secnumdepth}{0}

% Doc info
\author{Olivier St-Laurent, Maxime Daigle}
\title{IFT-3395  Devoir 3}
\date{2018-11-09}

\begin{document}

\maketitle

\section{Question 1 Relations et dérivées de quelques fonction de base}

\subsection{1. Montrez que sigmoid($x$) = $\frac{1}{2}(tanh(\frac{1}{2}x) + 1)$}

	$ sigmoid(x) = \frac{1}{2}(tanh(\frac{1}{2}x) + 1)  \iff  2sigmoid(x) - 1 = tanh(\frac{x}{2}) $ \\
\\
	$ 2sigmoid(x) - 1 = \frac{2-1-exp(-x)}{1 + exp(-x)} = \frac{1 - exp(-x)}{1+exp(x)}  = \frac{exp(\frac{x}{2})}{exp(\frac{x}{2})} (\frac{1 - exp(-x)}{1+exp(-x)})$ \\
\\
	$ = \frac{exp(\frac{x}{2}) - exp(\frac{x}{2})exp(-x)}{exp(\frac{x}{2}) + exp(\frac{x}{2})exp(-x)} = \frac{exp(\frac{x}{2}) - exp(\frac{-x}{2})} {exp(\frac{x}{2}) + exp(\frac{-x}{2})} = tanh(\frac{x}{2})$

\subsection{2. Montez que $\ln sigmoid(x)$ = $-softplus(-x)$}
	$ \ln sigmoid(x) = \ln \frac{1}{1 + exp(-x)} = \ln(1) - \ln(1+exp(-x)) = 0 + \ln (1+exp(-x)) = -softplus(-x)$
	
\subsection{3. Montrez que $\frac{d\text{ }sigmoid}{dx}(x) = sigmoid(x)(1-sigmoid(x)$}
	$ \frac{d\text{ }sigmoid}{dx}(x) = \frac{((1+exp(-x))^{-1})}{dx} =  \frac{-1}{(1 + e^{-x})^{2}}(-e^{-x})  = (\frac{1}{1 + e^{-x}})(\frac{e^{-x}}{1+e^{-x}})$ \\
\\
$ = sigmoid(x)(\frac{1 + e^{-x}}{1 + e^{-x}} - \frac{1}{1 + e^{-x}}) =  sigmoid(x)(1 - sigmoid(x))$

\subsection{4. Montrez que la dérivée de tanh est : $tanh'(x) = 1 - tanh^{2}(x)$}
	$ \frac{d}{dx}(\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}) =  \frac{(e^{x} - e^{-x})'(e^{x} + e^{-x})-(e^{x} - e^{-x})(e^{x} + e^{-x})'}{(e^{x}e^{-x})^{2}} = \frac{(e^{x}+e^{-x})^{2} - (e^{x}-e^{-x})^{2}}{(e^{x}+e^{-x})^{2}}$ \\
\\
$ = \frac{(e^{x}+e^{-x})^{2}}{(e^{x}+e^{-x})^{2}} - \frac{(e^{x}-e^{-x})^{2}}{(e^{x} + e^{-x})^{2}} = 1 - tanh^{2}(x)$

\subsection{5. exprimez la fonction sign en utilisant des fonctions indicatrices : sign(x) = \dots}

\[ 
sign(x) = 
\begin{cases} 
      1 &  x > 0 \\
      0 &  x = 0 \\
      -1 &  x < 0 
   \end{cases}
\Rightarrow sign(x) = \mathbb{1}_{\{x > 0\}}(x) - \mathbb{1}_{\{x < 0\}}(x)
\]

\subsection{6. Écrivez la dérivée de la fonction valeur absolue $abs(x) = |x|$}

$\forall x \in \mathbb{R}$,    $|x| = \sqrt{x^{2}} \Rightarrow \frac{d|x|}{dx} = \frac{d(x^{}2)^{\frac{1}{2}}}{dx} = \frac{1}{2}(x^{2})^{-\frac{1}{2}}2x = \frac{x}{\sqrt{}x^{2}} = \frac{x}{|x|}$ \\
\\
\[ 
|x| = 
\begin{cases} 
      x &  x \geq 0 \\
      -x &  x < 0 
   \end{cases}
\Rightarrow |x| = x*sign(x)
\]

$abs'(x) = \frac{x}{x*sign(x)} = \frac{1}{sign(x)}$ mais on veuxt que abs'(0) = 0. \\
Alors, on écrit abs'(x) = sign(x)

\subsection{7. Écrivez la dérivée de la fonction rect.}
\[ 
rect(x) = 
\begin{cases} 
      x &  x > 0 \\
     0 &  x \leq 0 
   \end{cases}
\]
Alors,
\[ 
rect'(x) = 
\begin{cases} 
     1 &  x > 0 \\
     0 &  x \leq 0 
   \end{cases}
\Rightarrow rect'(x) = \mathbb{1}_{\{x > 0\}}(x)
\]

\subsection{8. Soit le carré de la norme $L_{2}$ d<un vecteur: $||x||_{2}^{2}= \sum_{i}x_{i}^{2}.$. Écrivez le vecteur gradient : $\frac{\partial||x||_{2}^{2}}{\partial x} = \dots$}

\begin{eqs*}
	\frac{\partial \sum_{i}x_{i}^{2}}{\partial x} = 
	\begin{bmatrix}
			\frac{\partial (x_{1}^{2} + x_{2}^{2}+\dots+x_{n}^{2})}{\partial x_{1}}\\
			\vdots \\
			\frac{\partial (x_{1}^{2} + x_{2}^{2}+\dots+x_{n}^{2})}{\partial x_{n}}
	\end{bmatrix}
\end{eqs*}

\begin{eqs*}
	= 2
	\begin{bmatrix}
			x_{1}\\
			x_{2}\\
			\vdots \\
			x_{n}
	\end{bmatrix}
=2x
\end{eqs*}

\subsection{8. Soit la norme $L_{1}$ d<un vecteur :$||x||_{1} = \sum_{i}|x_{i}|$. Écrivez le vecteur de gradient : $\frac{\partial |x|_{1}}{\partial x} = \dots$}

\begin{eqs*}
	\frac{\partial \sum_{i}|x_{i}|}{\partial x} = 
	\begin{bmatrix}
			\frac{\partial (|x_{1}| + |x_{2}|+\dots+|x_{n}|)}{\partial x_{1}}\\
			\vdots \\
			\frac{\partial (|x_{1}| + |x_{2}|+\dots+|x_{n}|)}{\partial x_{n}}
	\end{bmatrix}
\end{eqs*}

\begin{eqs*}
	= 
	\begin{bmatrix}
			sign(x_{1})\\
			sign(x_{2})\\
			\vdots \\
			sign(x_{n})
	\end{bmatrix}
=sign(x)
\end{eqs*}


\section{Question 2 Calcul du gradient pour l'optimisation des paramètres d'un réseau de neurones pour la classification multiclasse}

\subsection{1. Exprimez le vecteur des sorties des neurones de la couche cachée $h^{s}$ en fonction de $h^{a}$.}

$b^{(1)} \in \mathbb{R}^{d_{h}}$ \\
$h^{a}=b^{(1)}+W^{(1)}$ \\

\begin{align*} 
  h^{a}
    &=
    \left(
      \begin{array}{r}
        b_{1}^{(1)} \\
          \vdots \\
          b_{d_{h}}^{(1)} \\
      \end{array}
     \right) + 
     \left(
       \begin{array}{rrrr}
         w_{11}^{(1)} &   w_{12}^{(1)} &   \dots &    w_{1d}^{(1)} \\
        \vdots &  \dots &   \dots&   \vdots \\
         w_{d_{h}}^{(1)} &   w_{d_{h}}^{(1)} &   \dots &    w_{d_{h}d}^{(1)}   \\
       \end{array}
      \right) 
      \left(
        \begin{array}{r}
        x_{1} \\
          \vdots \\
          x_{1} \\
        \end{array}
      \right)
\end{align*} \\
\begin{align*} 
     = \left(
       \begin{array}{r}
         b_{1}^{(1)}+w_{11}^{(1)}x_{1} +   w_{12}^{(1)}x_{2}  +   \dots  +    w_{1d}^{(1)}x_{d} \\
        \vdots\\
        b_{d_{h}}^{(1)}+w_{d_{h}1}^{(1)}x_{1} +   w_{d_{h}2}^{(1)}x_{2}  +   \dots  +    w_{d_{h}d}^{(1)}x_{d} \\
       \end{array}
      \right) 
      \left(
        \begin{array}{r}
        x_{1} \\
          \vdots \\
          x_{1} \\
        \end{array}
      \right)
\end{align*}
\\
$\Rightarrow h_{j}^{a}=b_{j}^{(1)}+\sum_{i=1}^{d}w_{ij}^{(1)}x_{i}$ \\
$h^{s}=rect(h^{a})$

\subsection{2.Donnez la formule de calcul du
vecteur d'activations des neurones de la couche de sortie $o^{a}$ à partir de
leurs entrées $h^{s}$ sous la forme d'une expression de calcul matriciel, puis
détaillez le calcul de $o_{a}^{k}$.}

$W^{(2)}$ est $m$ x $d_{h}$ et $b^{(2)} \in  \mathbb{R}^{m}$\\
$o^{a} = b^{(2)} + W^{(2)}h^{s}$ \\
\begin{align*} 
  o^{a}
    &=
    \left(
      \begin{array}{r}
        b_{1}^{(1)} \\
          \vdots \\
          b_{m}^{(2)} \\
      \end{array}
     \right) + 
     \left(
       \begin{array}{rrrr}
         w_{11}^{2} &   w_{12}^{2} &   \dots &    w_{1d_{h}}^{2} \\
        \vdots &  \dots &   \dots&   \vdots \\
         w_{m1}^{2} &   w_{m2}^{2} &   \dots &    w_{md_{h}}^{2}   \\
       \end{array}
      \right) 
      \left(
        \begin{array}{r}
        h_{1}^{s} \\
          \vdots \\
          h_{d1}^{s} \\
        \end{array}
      \right)
\end{align*}

$\Rightarrow o_{k}^{a} = b_{k}^{(2)} + \sum_{i=1}^{d_{h}}w_{ki}^{(2)}h_{i}^{s}$

\subsection{3. Démontrez que les $o_{k}^{s}$ sont positifs et somment à 1. Pourquoi est-ce important?}

$O_{k}^{s} = softmax(O^{a})_{k} = \frac{exp(O_{k}^{a})}{\sum_{i=1}^{m}exp(O_{i}^{a})}$ \\
les $O_{k}^{s}$ sont positifs par définitions de exp(x) (i.e $\forall x \in \mathbb{R}, exp(x) > 0$)\\

$\sum_{k=1}^{m}O_{k}^{s} = \sum_{k=1}^{m}\frac{exp(O_{k}^{a})}{\sum_{i=1}^{m}exp(O_{i}^{a})}$\\
$ = \frac{\sum_{k=1}^{m}exp(O_{k}^{a})}{\sum_{i=1}^{m}exp(O_{i}^{a})} = 1$

Il est important que les $O_{k}^{s}$ soient positif et qu'ils somment à 1, car cela permet d'interpréter $O_{k}^{s}$ commpe P(Y = k|X=x) (c'est-à-dire
qu'on interprète $O_{k}^{s}$ comme étant la probabilité que l'entrée x soit de la classe k)

 \subsection{4. $L(x,y) =  - log(O_{y}^{s}(x)) Préciser L en fonction de O^{a}$}

$L(x,y) = -log( \frac{exp(O_{k}^{a})}{\sum_{i=1}^{m}exp(O_{i}^{a})}) = log(\sum_{i=1}^{m}exp(O_{i}^{a})) - log(exp(O_{y}^{a}))$\\
$= log(\sum_{i=1}^{m}exp(O_{i}^{a})) - O_{y}^{a}$


 \subsection{5. Formuler $\hat{R}$. Indiquer précisement l'ensemble $\theta$ des paramètre du réseau. Indiquer à combien de paramètres scalaires $n_{\theta}$ cela correspond. Formuler le problème d'optimisation qui correspond à l;entrainement du réseau pour trouver une valeur optimal des paramètres.}

$\hat{R} = \frac{1}{n}\sum_{i=1}^{n}L(x^{(1), y^{(1)}}) = \frac{1}{n}\sum_{i=1}^{n}(log(\sum_{j=1}^{m}exp(O_{j}^{a}(x^{(i)}))-O_{y^{i}}^{a}(x^{(i)}))$\\

$\theta = \{W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}\}$\\

$n_{\theta} = d_{h}\text{ x }d + d_{h} \text{ x } d_{h} +m$ \\

Le problème d'optimisation qui correspond à l'entrainement du réseau permettrant de trouver une valeur optimale des paramètre est $argmin\hat{R}(\theta, D_{train})$

 \subsection{6. Exprimer avec un bref pseudo-code la descente de gradient pour ce problème}
Initialize $\theta$\\
for N iteration:\\
$\theta \leftarrow \theta-n(\frac{1}{n}\sum_{i=1}^{n}\frac{d}{d\theta}(log(\sum_{j=1}^{m}exp(O_{j}^{a}(x^{(i)}))-O_{y^{i}}^{a}(x^{(i)})))$


 \subsection{7. Montrez que $\frac{dL}{dO^{a}} = O^{s} - onehot_{m}(y)$}
Pour k $\neq$ y,\\

$\frac{\partial L(x,y)}{dO_{k}^a} = \frac{\partial (log(sum_{i=1}^{m}exp(o_j^a)) - O_y^a)}{\partial O_k^a} = \frac{\partial log(\sum_{j-1}^m exp(o_j^a))}{\partial O_k^a} $\\
$= \frac{1}{\sum_{j-1}^m exp(o_j^a)}*\frac{\partial \sum_{j-1}^m exp(o_j^a)}{\partial O_k^a} = \frac{exp(O_k^a)}{\sum_{j-1}^m exp(o_j^a)} = O_k^s$\\
\\
$\frac{\partial L(x,y)}{\partial O_y^a} = \frac{(log(\sum_{j=1}^{m}exp(O_{j}^{a})-O_y^a)}{\partial O_y^a} = \frac{exp(O_y^a)}{\sum_{j=1}^{m}exp(O_{j}^{a}} -1 =O_y^s -1 $\\
\\

Alors,

\begin{align*} 
  \frac{\partial L(x,y)}{\partial O^a}
    &=
    \left(
      \begin{array}{r}
       \frac{\partial L}{\partial O_1^a} \\
          \dots \\
          \frac{\partial L}{\partial O_m^a} \\
      \end{array}
     \right)
\end{align*}

\begin{align*} 
    &=
    \left(
      \begin{array}{r}
       O_1^s \\
          \dots \\
	O_y^s\\
	\dots\\
          O_m^s\\
      \end{array}
     \right) - 
	\left(
      \begin{array}{r}
       O \\
          \dots \\
	1\\
	\dots\\
          O\\
      \end{array}
     \right)
\end{align*}
$ = O^s - onehot_m(y)$

 \subsection{8 Donner l'expression correspondante en numpy}
$grad\_oa = os - np.eye(m)[y-1]$\\
car $y \in \{1,\dots,m\}$ et le vecteur $onehot_m$ à des index de O à m-1

 \subsection{9 calculer $\frac{\partial L}{\partial W^{(2)}}$ et $\frac{\partial L}{\partial b^{(2)}}$}
pour $k \neq y$,

$\frac{\partial L(x,y)}{\partial W_{kj}^{(2)}} = o_k^s*\frac{\partial O_k^a}{\partial W_{kj}^{(2)}}=O_k^s*\frac{\partial (b_k^{(2)} + \sum_{i=1}^{d_h}W_{ki}^{(2)}h_i^s)}{\partial Wk_{kj}^{(2)}} = o_k^s*h_j^s$\\

pour $k=y$,
$\frac{\partial L(x,y)}{\partial W_{kj}^{(2)}} = (O_y^s-1)*\frac{\partial O_y^a}{\partial W_{yj}^{(2)}} = (O_y^s-1)*h_j^s=O_y^sh_j^s-h_j^s$\\

pour $k \neq y$,
$\frac{\partial L(x,y)}{\partial b_k^{(2)}} = O_k^s*\frac{\partial (b_k^{(2)} + \sum_{i=1}^{d_h}W_{ki}^{(2)}h_i^s)}{\partial b_k^{(2)}} = o_k^s$

pour $k = y$,
$\frac{\partial L(x,y)}{\partial b_k^{(2)}} = O_k^s - 1$\\
Alors,\\
\begin{align*} 
    \frac{\partial L}{\partial W^{(2)}}
&=
     \left(
       \begin{array}{rrrr}
         O_1^sh_1^s &   O_1^sh_2^s &   \dots &   O_1^sh_{d_h}^s \\
        \vdots &  \dots &   \dots&   \vdots \\
         O_m^sh_1^s &   O_m^sh_2^s &   \dots &   O_m^sh_{d_h}^s \\
       \end{array}
      \right)  - 
	\left(
     \begin{array}{rrrr}
         O &   \dots &   \dots &   O\\
        \vdots &  \dots &   \dots&   \vdots \\
         h_1^s & h_2^s & \dots &h_{d_h}^s \\
 	\vdots &  \dots &   \dots&   \vdots \\
	O &   \dots &   \dots &   O\\
       \end{array}
     \right)
\end{align*}



\begin{align*} 
    \frac{\partial L}{\partial b^{(2)}}
&=
     \left(
       \begin{array}{r}
         O_1^s\\
        \vdots \\
         O_y^s\\
	\vdots\\
	O_m^s\\
       \end{array}
      \right)  - 
	\left(
     \begin{array}{r}
         0\\
        \vdots \\
         0\\
	\vdots\\
	0\\
       \end{array}
     \right)=o^s-onehot_m(y)
\end{align*}

 \subsection{10 Donner les expression correspondantes en numpy}
$grad\_b2 = os - np.eye(m)[y-1]$\\
$grad\_W2 = np.outer(os,hs)-np.concatenate((np.zeroes((y-1,dh)),hs.reshape(1,dh),np.zeroes((m-y,dh))))$\\

grad\_b2 est m x 1\\
grad\_W2 est m x $d_h$\\

car $\frac{\partial L}{\partial b^{(2)}} = o^s-onehot_m()y$ et $frac{\partial L}{\partial W^{(2)}}$ \\


\begin{align*} 
&= o^sh^{s^t} - 
	\left(
     \begin{array}{rrrr}
         O &   \dots &   \dots &   O\\
        \vdots &  \dots &   \dots&   \vdots \\
         h_1^s & h_2^s & \dots &h_{d_h}^s \\
 	\vdots &  \dots &   \dots&   \vdots \\
	O &   \dots &   \dots &   O\\
       \end{array}
     \right)
\end{align*}

où $O^s$ et $onehot_m(y)$ sont m x 1, $h^s$ est $d_h$ x $1$ et la matrice contenant que des zéros et les /l/ments de $h^s$ est m x $d_h$

 \subsection{11 Calculer $\frac{\partial L}{\partial h^s}$}
$ \frac{\partial L}{\partial h_j^s} = \sum_{k=1}^m \frac{\partial L}{\partial O_k^a}\frac{\partial O_k^a}{\partial h_j^s} = \sum_{k=1}^m \frac{\partial L}{\partial O_k^a}\frac{\partial (b^{(2)}+\sum_{i=1}^{d_h}W_{ki}^{(2)}h_i^s)}{\partial h_j^s}$\\
$ = \sum_{k=1}^m\frac{\partial L}{\partial O_k^a}W_{kj}^{(2)} =  O_1^sW_{1j}^{(2)}+O_2^sW_{2j}^{(2)}+\dots+(O_y^s-1)W_{yj}^{(2)}+\dots+O_m^sW_{mj}^{(2)}$\\

$=\sum_{k=1}^mO_k^sW_{kj}^{(2)}-W_{yj}^{(2)}$\\
Alors,\\

\begin{align*} 
\frac{\partial L}{\partial h^s}&= 
	\left(
     \begin{array}{r}
         \sum_{k=1}^m O_k^s W_{k1}^{(2)} - W_{y1}^{(2)}\\
 	\sum_{k=1}^mO_k^sW_{k2}^{(2)}-W_{y2}^{(2)}\\
        \dots  \dots\\ 
 	\sum_{k=1}^mO_k^sW_{kd_h}^{(2)}-W_{yd_h}^{(2)}\\
       \end{array}
     \right)
\end{align*}

 \subsection{12 Exprimer sous forme matricielle, préciser les dimensions, en numpy grad\_hs=?}

$\frac{\partial L}{\partial h^s} = W^{(2)^{t}}O^s-W^{(2)}[y,:]^t$\\

où $W^{(2)^t}$ est $d_h$ x $m$, $O^s$ est m x 1 et $w^{(2)}[y,:]^t$ est $d_h$ x $1$\\

$grad\_hs = W2^t.os-W@[y-1,:]^t$\\


 \subsection{13 Calculer $\frac{\partial L}{\partial h^a}$}

$\frac{\partial L}{\partial h_j^a} = \frac{\partial L}{\partial h_j^s}*\frac{\partial h_j^s}{\partial h_j^a} =(\sum_{k-=1}^m O_k^s W_{kj}^{(2)} - W_{yj}^{(2)})\frac{\partial (rect(h_j^a)}{\partial h_j^a} =  (\sum_{k=1}^m O_k^s W_{kj}^{(2)}-W_{yj}^{(2)})\mathbb{1}_{\{h_j^a > 0\}}(h_j^a)$\\



Alors,\\
\begin{align*} 
\frac{\partial L}{\partial dh^a}&= 
	\left(
     \begin{array}{r}
         (\sum_{k=1}^m O_k^s W_{k1}^{(2)}-W_{y1}^{(2)})\mathbb{1}_{\{h_1^a > 0\}}(h_1^a)\\
        \dots  \dots\\ 
 	 (\sum_{k=1}^m O_k^s W_{kd_{h}}^{(2)}-W_{yd_{h}}^{(2)})\mathbb{1}_{\{h_{d_h}^a > 0\}}(h_{d_h}^a)\\
       \end{array}
     \right)
\end{align*}

 \subsection{14 Exprimer sous forme matricielle, préciser les informations, donner l'équivalent en numpy}



\begin{align*} 
\frac{\partial L}{\partial h^a} &= 
	\left(
     \begin{array}{r}
	\frac{\partial L}{\partial h^s}
       \end{array}
     \right) \odot
\left(
     \begin{array}{r}
	\mathbb{1}_{\{h_1^a > 0\}}(h_1^a)\\
	\dots\\
	\mathbb{1}_{\{h_{d_h}^a > 0\}}(h_{d_h}^a)\\
       \end{array}
     \right)
\end{align*}

où $\frac{\partial L}{\partial h^a}$, $\frac{\partial L}{\partial h^s}$ et le vecteur contenant les fonctions indicatrices sont $d_h$ x 1\\

$vector\_indicator = np.array([1 id e> 0 esle 0 for e in hs])$\\
$grad\_ha = np.multiply(grad\_hs, vector_indicator)$

 \subsection{15 Calculer $\frac{\partial L}{\partial W^{(1)}}$ et $\frac{\partial L}{\partial b^{(1)}}$}

$\frac{\partial L}{\partial W_{jl}^{(1)}} = \frac{\partial L}{\partial h_j^a}\frac{\partial h_j^a}{\partial W_{jl}^{(1)}} = \sum_{k=1}^m O_k^s W_{kj}^{(2) - W_{yj}^{(2)}}\mathbb{1}_{\{h_j^a>0\}}(h_j^a)*\frac{\partial (b_j^{(1)} + \sum_{i=1}^d W_{ji}^{(1)}x_i} {\partial W_{jl}^{(1)}}$\\

$= (\sum_{k=1}^m O_k^s W_{kj}^{(2)}- W_{yj}^{(2)})\mathbb{1}_{\{h_j^a>0\}}(h_j^a)x_l = \frac{\partial L}{\partial h_j^a}*x_l$\\

$\frac{\partial L}{\partial b_j^{(1)}} = \frac{\partial L}{\partial h_j^a}*\frac{\partial h_j^a}{\partial b_j^{(1)}} = \frac{\partial L}{\partial h_j^a}*1 =  \frac{\partial L}{\partial h_j^a}$\\

\begin{align*} 
\frac{\partial L}{\partial W^{(1)}}
	&= 
	\left(
     \begin{array}{rrrr}
         \frac{\partial L}{\partial h_1^a}x_1 &  \frac{\partial L}{\partial h_1^a}x_2&   \dots &   \frac{\partial L}{\partial h_1^a}x_d\\
         \frac{\partial L}{\partial h_2^a}x_1 &  \frac{\partial L}{\partial h_2^a}x_2 &   \dots &   \frac{\partial L}{\partial h_2^a}x_d\\
         \vdots & \dots & \dots & \vdots \\
	 \frac{\partial L}{\partial h_{d_{h}}^a}x_1 &  \frac{\partial L}{\partial h_{d_{h}}^a}x_2 &   \dots &   \frac{\partial L}{\partial h_{d_{h}}^a}x_d\\
       \end{array}
     \right)
\end{align*}

$\frac{\partial L}{\partial b^{(1)}} = \frac{\partial L}{\partial h^a}$


 \subsection{16 Exprimer sous forme matricielle, définir les dimensions, donner l'équivalent en numpy}

$\frac{\partial L}{\partial b^{(1)}} = \frac{\partial L}{\partial h^a}$ est $d_h$ x 1\\

$\frac{\partial L}{\partial W^{(1)}} = \frac{\partial L}{\partial h^a} * x^t$ est $d_h$ x d car $\frac{\partial L}{\partial h^a}$ est $d_h$ x 1 et $x^t$ est 1 x d\\

$grad\_b1 = grad\_ha$\\
$grad\_W1 = np.outer(grad\_ha, x)$\\

 \subsection{17 Calculer $\frac{\partial L}{\partial x}$}

$\frac{\partial L}{\partial x_l} = \sum_{j=1}^{d_h} \frac{\partial L}{\partial h_j^a}\frac{\partial h_j^a}{\partial x_l} = \sum_{j=1}^{d_h}\frac{\partial L}{\partial h_j^a} \frac{\partial (b_j^{(1)} + \sum_{i=1}^d W_{ji}^{(1)}x_i)}{\partial x_l} $\\

$= \sum_{j=1}^{d_h} (\sum_{k=1}^m O_k^s W_{kj}^{(2)}-W_{yi}^{(2)})\mathbb{1}_{\{h_j^a>0\}}(h_j^a)W_{jl}^{(1)}$\\

Alors,

\begin{align*} 
\frac{\partial L}{\partial x}&= 
	\left(
     \begin{array}{r}
         ( \sum_{j=1}^{d_h} (\sum_{k=1}^m O_k^s W_{kj}^{(2)}-W_{yi}^{(2)})\mathbb{1}_{\{h_j^a>0\}}(h_j^a)W_{j1}^{(1)}\\
        \dots  \dots\\ 
 	 ( \sum_{j=1}^{d_h} (\sum_{k=1}^m O_k^s W_{kj}^{(2)}-W_{yi}^{(2)})\mathbb{1}_{\{h_j^a>0\}}(h_j^a)W_{jd}^{(1)}\\
       \end{array}
     \right)
\end{align*}

\begin{align*} 
&= 
	\left(
     \begin{array}{r}
        \sum_{j=1}^{d_h}\frac{\partial L}{\partial h_j^a}*W_{j1}^{(1)}\\
        \dots  \dots\\ 
 	 \sum_{j=1}^{d_h}\frac{\partial L}{\partial h_j^a}*W_{jd}^{(1)}\\
       \end{array}
     \right)
\end{align*}


 \subsection{18 Comment minimiser $\tilde{R} = \hat{R} + \lambda_{11}(\sum_{i,j}|W_{ij}^{(1)}|) + \lambda_{12}(\sum_{i,j}(W_{ij}^{(1)})^2)
+  \lambda_{21}(\sum_{i,j}|W_{ij}^{(2)}|) + \lambda_{22}(\sum_{i,j}(W_{ij}^{(2)})^2)$ au lieu de $\hat{R}$ change le gradient par le rappot aux différents paramètre?}

Il n'y a pas de différence pour $b^{(1)}$ et $b^{(2)}$ car $ \frac{\partial \zeta(\Theta)}{\partial b^{(1)}} =\frac{\partial \zeta(\Theta)}{\partial b^{(2)}} = 0 $


\begin{align*} 
\frac{\partial \zeta(\Theta)}{\partial W^{(1)}} &= \lambda_{11}
	\left (
     \begin{array}{rrrr}
       sign(W_{11}^{(1)}) & sign(W_{12}^{(1)}) & \dots & sign(W_{1d}^{(1)})\\
        sign(W_{11}^{(1)}) & \dots  & \dots & \vdots\\ 
 	sign(W_{d_{h}1}^{(1)}) & sign(W_{d_{h}2}^{(1)}) & \dots & sign(W_{d_{h}d}^{(1)})\\
       \end{array}
     \right) + \lambda_{12}
	\left (
     \begin{array}{rrrr}
       2W_{11}^{(1)} & 2W_{12}^{(1)} & \dots & 2W_{1d}^{(1)}\\
        2W_{11}^{(1)} & \dots  & \dots & \vdots\\ 
 	2W_{d_{h}1}^{(1)} & 2W_{d_{h}2}^{(1)} & \dots & 2W_{d_{h}d}^{(1)}\\
       \end{array}
     \right)
\end{align*}

$ = \lambda_{11} sign(W^{(1)}) + 2\lambda_{12}W^{(1)}$ \\

Alors,\\
$\frac{\partial \tilde{R}}{\partial W^{(1)}} + \lambda_{11} sign(W^{(1)}) + 2\lambda_{12}W^{(1)}$\\

et de la même façon, $\frac{\partial \tilde{R}}{\partial W^{(2)}} + \lambda_{21} sign(W^{(2)}) + 2\lambda_{22}W^{(2)}$ 













\end{document}