\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[francais]{babel}
\usepackage{amsmath, amssymb}

\newcommand\ra\rightarrow
\newcommand\la\leftarrow
\newcommand\lra\leftrightarrow
\newcommand\ua\uparrow
\newcommand\da\downarrow
\newcommand\uda\updownarrow
\newcommand\nea\nearrow

\newcommand\Ra\Rightarrow
\newcommand\La\Leftarrow
\newcommand\Lra\Leftrightarrow

\newcommand\mat[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand\smat[1]{\setstretch{1}{\ensuremath{\scalefont{0.8}\mat{#1}}}}

\newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
\newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
\newcommand{\bracket}[2]{\ensuremath{\left\langle #1 \middle| #2 \right\rangle}}
\newcommand{\matrixel}[3]{\ensuremath{\left\langle #1 \middle| #2 \middle| #3 \right\rangle}}
\newenvironment{eq*}{\begin{equation*}\begin{gathered}}{\end{gathered}\end{equation*}}
\newenvironment{eqs*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}

% Paragraph formatting
\setlength{\parindent}{0pt}
\usepackage{setspace}
\onehalfspacing
% Margin
\usepackage[margin=1in]{geometry}
\setcounter{secnumdepth}{0}

% Doc info
\author{Olivier St-Laurent, Maxime Daigle}
\title{IFT-3395  Devoir 1}
\date{2018-09-27}

\begin{document}

\maketitle

\section{Question 1}

Nous pensons que la conclusion des médecins est fausse. En réalité, puisque la probabilité à priori d'avoir le cancer est très faible (1.5\%), la probabilité de ne pas être atteint du cancer, en sachant que le test est positif, est largement supérieure à la probabilité d'être atteinte.
\\[\baselineskip]
Si nous calculons la probabilité exacte avec la loi de Bayes, nous avons:
\begin{eqs*}
P(cancer | +) = \tfrac{P(+ | cancer) * P(cancer)}{P(+ | cancer) * P(cancer) + P(+ | cancer^c) * P(cancer^c)}
\end{eqs*}
\\[\baselineskip]
En remplaçant dans l'équation les valeurs que nous avons, nous obtenons:
\begin{eqs*}
P(cancer | +) = \tfrac{(0.87)(0.015)}{(0.87)(0.015) + (0.096)(1 - 0.015)} = 0.12\%
\end{eqs*}

\section{Question 2}
\subsection{2.1}
Si pour 2 dimensions, l'aire d'un carré de côté $c$ est donné par $c^2$ et que pour 3 dimensions, le volume d'un cube de côté $c$ est donné par $c^3$, alors
on peut généralisé que le volume d'un hyper-cube de coté $c$ en dimension $d$ est donné par $V = c^d$


\subsection{2.2}

Pour une variable aléatoire $X$ uniformément distribuer sur un interval [$\alpha$, $\beta$] de dimension 1, sa fonction de densité de probabilité est donnée par:
\[
    f(x) =
	\begin{cases}
        \tfrac{1}{\beta -\alpha}, & \text{si $\alpha \leq x \leq \beta$} \\
        0, & \text{ sinon}
	\end{cases}
\]
Pour une variable aléatoire vectorielle $X$ de 2 dimensions uniformément distribuée sur une région $R$, elle aussi de deux dimension, la fonction
de densité conjointe est une constante $c$ pour tout point dans $R$ et 0 pour tout point a l'extérieur de $R$
\[
    f(x_{1}, x_{2}) =
	\begin{cases}
        c, & \text{si $(x_{1}, x_{2}) \in R$}  \\
        0, & \text{ sinon}
	\end{cases}
\]

Si on généralise le cas pour $n$ dimensions, on obtient la fonction de densité conjointe:
\[
    f(x_{1}, ...,x_{d}) =
	\begin{cases}
        c, & \text{si $(x_{1}, ..., x_{d}) \in R^d$}  \\
        0, & \text{ sinon}
	\end{cases}
\]

La probabilité $p(x)$ pour une valeur exacte dans un problème de probabilité continue est de 0, car l'intervale est infiniment petit.
\\[\baselineskip]
En effet, la probabilité $p(x)$ dans $d$ dimension peut être calculé par:

\begin{eqs*}
p(x) = \int_{\alpha_{1}}^{\beta_{1}} ... \int_{\alpha_{d}}^{\beta_{d}} f(x_{1}, ...,x_{d})dx_{1}... dx_{d} = c*\text{Volume de }R^d
\end{eqs*}

\subsection{2.3}
Si la  bordure est de 3\%, la probabilité que $x_{i}$, $i \in 1, ..., d$ se retrouve dans cette zone est de $1 - 0.94 = 0.06$ Donc pour un $x$ de l'hyper-cube, la
probabilité de tomber dans le cube et pas dans la bordure est donnée par l'équation:
\begin{eqs*}
p(x) = \int_{0}^{0.94} ... \int_{0}^{0.94}dx_{1}... dx_{d} = \tfrac{0.94^d}{1}
\end{eqs*}
et la probabilité de tomber dans la bordure est de:
\begin{eqs*}
1 - p(x) =1 - \tfrac{0.94^d}{1}
\end{eqs*}

\subsection{2.4}
Pour $d$ = 1
\begin{eqs*}
1 - p(x) =1 - \tfrac{0.94^1}{1} = 0.06
\end{eqs*}

Pour $d$ = 2
\begin{eqs*}
1 - p(x) =1 - \tfrac{0.94^2}{1} = 0.1164
\end{eqs*}

Pour $d$ = 3
\begin{eqs*}
1 - p(x) =1 - \tfrac{0.94^5}{1} = 0.1694
\end{eqs*}

Pour $d$ = 5
\begin{eqs*}
1 - p(x) =1 - \tfrac{0.94^5}{1} = 0.2661
\end{eqs*}

Pour $d$ = 10
\begin{eqs*}
1 - p(x) =1 - \tfrac{0.94^{10}}{1} = 0.4613
\end{eqs*}

Pour $d$ = 100
\begin{eqs*}
1 - p(x) =1 - \tfrac{0.94^{100}}{1} = 0.9979
\end{eqs*}

Pour $d$ = 1000
\begin{eqs*}
1 - p(x) =1 - \tfrac{0.94^{1000}}{1} = 1 - 1.3423*10^{-27}
\end{eqs*}

\newpage
\subsection{2.5}

Plus la dimensionalité est grande, plus la probabilité de tomber dans la bordure augmente. Si on y pense, c'est logique, car lorsque la dimensionalité est de 
100, par exemple, pour tomber dans la bordure, il suffit qu'une seule des 100 dimensions soit dans celle-ci pour ne plus être dans l'hyper-cube. 

\section{Question 3}
Pour la question 3, on considère un ensemble de données $D = \{x^{(1)}, ..., x^{(n)}\}$ avec $x \in \mathbb{R}^{d}$
\subsection{3.1 Les paramètres d’une densité Gaussienne isotropique sur D}

\subsubsection{a) - Nommez ces paramètres et indiquez-en les dimensions.}
$\mu$: est la moyenne et elle est de dimention t$d$ \\
$\sigma$: est la variance et elle est de dimension 1 \\
$\Sigma = \sigma^{2}I$ est la matrice de covariance et elle est de dimension $[d \text{, } d]$. 

\subsubsection{b) - Exprimez en fonction des points de D la formule qui nous donnera la valeur des paramètres optimaux.}
\begin{eqs*}
\mu = \tfrac{1}{n}\sum_{t=1}^{n} x^{(t)}
\end{eqs*}

\begin{eqs*}
\Sigma_{ij} = \tfrac{1}{n}\sum_{t=1}^{n}(x_{i}^{(t)} - \mu_{i})(x_{j}^{(t)} - \mu_{j})
\end{eqs*}

\begin{eqs*}
\Sigma= \tfrac{1}{n}\sum_{t=1}^{n}(x^{(t)} - \mu)(x^{(t)} - \mu)^{'}
\end{eqs*}

\subsubsection{c) - Quelle est la complexité algorithmique de cet apprentissage?}

Pour calculer $\mu: O(n*d)$, car calculer $\mu_{i}$ est $O(n)$ et $O(d)$ pour l'espace mémoire. \\
Pour calculer $\Sigma_{ij}: O(n)$ \\ 
et pour calculer $\Sigma:O(n+d)$ car puisque nous sommes dans un cas isotropique, $\Sigma = \sigma^{2}I$. Cela signifie qu'on doit calculer
$\sigma^{2}$ une fois pour ensuite créer une matrice diagonale de $d$ éléments non nuls. Si ce n'était pas un cas isotropique, la complexité algorithmique de $\Sigma$ serait $O(n*d)$

\subsubsection{d) - Pour un point de test x, écrivez la fonction qui donnera la densité de probabilité prédite au point x}
\[
	\hat{p}(x) = \tfrac{1}{2\pi^{\frac{d}{2}}\sqrt{\left|\Sigma\right|}} * exp(-\frac{1}{2}(x - \mu)^{T}\Sigma^{-1}(x - \mu))
\]
\[
	= \tfrac{1}{2\pi^{\frac{d}{2}}\sigma^{d}} * exp(-\frac{1}{2}\frac{||x - \mu ||^{2}}{\sigma^{2}})
\]

où
\[\mu = \tfrac{1}{n}\sum_{t=1}^{n} x^{(t)}\]
et
\[
\Sigma = \tfrac{1}{n}\sum_{t=1}^{n}(x^{(t)} - \mu)(x^{(t)} - \mu)^{'} \\
= \sigma^{2}I
\]

\subsubsection{e) - Quelle est la complexité algorithmique pour le calcul de cette prédiction à chaque nouveau point x}
Pour calculer la complexité est $\hat{p}:O(n*d + n +  d + d)$ Si ont doit calculer $\mu$ et $\sigma$ préalablement. Sinon,
la complexité, losqu'on connait déjà  $\mu$ et $\sigma$ est de $O(d)$
\\
JE VOIS MAL LA FIN DE LA FEUILLE ET JE COMPREND PAS LES CALCULS
\subsection{3.2 Les fenêtre de Parzen}
\subsubsection{a) - $\sigma$ fixé, en quoi consiste le training par les fenêtres de Parzen}
\begin{eqs*}
	K(X_{i}, x) = \tfrac{1}{2\pi^{\frac{d}{2}}\sigma^{d}} * exp(-\frac{1}{2}\frac{d(X_{i},x)^{2}}{\sigma^{2}})
\end{eqs*}
\begin{eqs*}
	= \mathcal{N}_{x_{i}, \sigma^{2}}(x)
\end{eqs*}

où $x_{i}$ est QUELQUE CHOSE POINTER PAR UNE FLECHE et $x$ est le point au centre de la zone QUELQUE CHOSE PAS CAPABLE DE LIRE
\begin{eqs*}
	= \mathcal{N}_{x, \sigma^{2}}(x_{i})
\end{eqs*}

On a que
\begin{eqs*}
	f(x) = \hat{p} = \frac{1}{n}\sum_{i=1}^{n} K(X_{i}, x)
\end{eqs*}
\begin{eqs*}
	= \frac{1}{n}\sum_{i=1}^{n} \mathcal{N}_{x_{i}, \sigma^{2}}(x)
\end{eqs*}

Puisqu'il n,y a pas de paramètres à entrainer/estimer, il n'y a pas d'entrainement à faire. Nous devons simplement stocker les données d'apprentissage.
C'est lorsqu'on veut calculer pour un nouveau point que les calculs sont faits.

\subsubsection{b) - Écrivez en une seule formule la fonction qui donnera la densité de probabilité prédite au point x}
\begin{eqs*}
	\hat{p}(x) = \frac{1}{n}*\frac{1}{(2\pi)^{\frac{d}{2}}\sigma^{d}} \sum_{i=1}^{n} exp(-\frac{1}{2}*\frac{d(X_{i}, x)^{2}}{\sigma^{2}})
\end{eqs*}

\subsubsection{c) - Quelle est la complexité algorithmique pour le calcul de cette prédiction à chaque nouveau point x}
La complexité est de $O(n)$, car les mesure de distance vi sont $L_{2},  L_{1},  L_{????},  L_{p},  L_{????}$.
Donc, $d(a,b) = (\sum_{i=1}^{d}|a_{i} - b_{i}|^{????})^{\frac{1}{p}}$ est en $O(d)$  

\subsection{3.3 Capacité}
\subsubsection{(a)}
Le Parzen à noyau Gaussien a la plus forte capacité étant donné qu'il peut exprimer l'équivalent de plusieurs densité paramétriques Gaussienne. 
\\
METTRE UN PEU PLUS D'EXPLICATION. E.G DEFINITION DE LA CAPACITER
\subsubsection{(b)}
Avec les fenêtre de Parzen à noyau Gaussien, il y a de plus forte chance d'avoir du sur-apprentissage. En effet, quand $\sigma$ est très proche de zéro, la capacité se veut largement trop forte relativement à la quantité de données. Cela va créer beaucoup de piques autour des points de données. 

\subsubsection{(c)}
Les fenêtres de Parzen sont non-paramétrique, c'est-à-dire qu'il n'y a pas de supposition sur la forme de la fonction cible. Tandis que, pour la densité paramétrique Gaussienne, on suppose que la fonction de densité est connue et on cherche à estimer les paramètre de la distribution. En d'autre mots, l'utilisateur doit fixer le $\sigma$ lorsqu'il utilise les fenêtre de parzen et il doit valider sont choix en utilissant la méthode de validation croisée. Dans l'autre cas, rien n'est fixée par l'utilisateur,  $\sigma$ est inféré par l'algorithme.
\end{document}
